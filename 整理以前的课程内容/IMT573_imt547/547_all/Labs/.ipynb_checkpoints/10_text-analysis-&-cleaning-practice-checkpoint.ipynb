{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics with NLTK\n",
    "\n",
    "### What is NLTK?\n",
    "The Natural Language Toolkit (NLTK) is a Python package for natural language processing.\n",
    "NLTK comes with many corpora, toy grammars, trained models, etc. https://www.nltk.org/data.html\n",
    "\n",
    "Let's start by downloading nltk. See https://www.nltk.org/data.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json, io #https://pypi.org/project/nltk/ --- pip install nltk \n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Read documentation: https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "* sentence tokenization\n",
    "* word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Netanyahu's visit was cut short by reports late Sunday that a rocket was fired from Gaza into central Israel, wounding at least seven people. Following criticism from political opponents over what they consider the prime minister's unclear stance toward the militant political group, Israel responded with a series of strikes into Gaza against Hamas, which largely governs the contested strip. President Donald Trump tacitly endorsed the strike following his meetings with Netanyahu, calling the Hamas attack \\\"despicable.\\\"\"\n",
    "\n",
    "# tokenize text into words\n",
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sentence tokenizer**\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "\n",
    "Punkt Sentence Tokenizer\n",
    "\n",
    "The NLTK data package includes a pre-trained Punkt tokenizer for English. \n",
    "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into sentences\n",
    "sents = sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent_tokenize is quite smart. See examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.  And sometimes sentences can start with non-capitalized words.  i is a good variable name.\"\n",
    "\n",
    "sents2 = sent_tokenize(text2)\n",
    "sents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = '''\n",
    "... (How does it deal with this parenthesis?)  \"It should be part of the\n",
    "... previous sentence.\" \"(And the same with this one.)\" ('And this one!')\n",
    "... \"('(And (this)) '?)\" [(and this. )]\n",
    "... '''\n",
    "\n",
    "\n",
    "sents3 = sent_tokenize( text3 )\n",
    "sents3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WhitespaceTokenizer\n",
    "\n",
    "```WhitespaceTokenizer```\n",
    "Tokenize a string on whitespace (space, tab, newline). In general, users should use the string split() method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "print(s)\n",
    "\n",
    "print('\\nWhitespaceTokenizer:')\n",
    "WhitespaceTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPunctTokenizer\n",
    "```WordPunctTokenizer``` Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp ```\\w+|[^\\w\\s]+```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WordPunctTokenizer:')\n",
    "WordPunctTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span class=\"mark\">TODO</span>**\n",
    "\n",
    "Try to tokenize tweets with the `tweetokenizer`\n",
    "https://www.nltk.org/api/nltk.tokenize.html?highlight=word_tokenize\n",
    "\n",
    "`tweetokenizer`: a Tokenizer specifically suited for tweets.\n",
    "\n",
    "You can start with any sample tweets.\n",
    "\n",
    "Here is one example: \"@remy This is waaaaayyyy too much for you!!!!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with whitespace tokenizer\n",
    "WhitespaceTokenizer().tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span class=\"mark\">TODO</span>**\n",
    "\n",
    "Test to see how two different tokenizers would function when you pass the same text. \n",
    "\n",
    " * Test with the same tweet that you picked as your sample data previously\n",
    " * Test with 2 tokenizers from NLTK: ```word_tokenize``` and ```casual_tokenize```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing tokenizers. \n",
    "Refer to this great resource: https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7\n",
    "\n",
    "Instead of taking the time to analyze the outcome of each tokenizer, we can put everything in one pd.dataframe for fast and accurate interpretation. How would you do it?\n",
    "\n",
    "**<span class=\"mark\">TODO for later</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopeng = set(stopwords.words('english'))\n",
    "stopeng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from text\n",
    "\n",
    "#text = \"Netanyahu's visit was cut short by reports late Sunday that a rocket was fired from Gaza into central Israel, wounding at least seven people. Following criticism from political opponents over what they consider the prime minister's unclear stance toward the militant political group, Israel responded with a series of strikes into Gaza against Hamas, which largely governs the contested strip. President Donald Trump tacitly endorsed the strike following his meetings with Netanyahu, calling the Hamas attack \\\"despicable.\\\"\"\n",
    "text = \"NASA Mars Rover Is Bringing 10.9 Million Names to the Red Planet\"\n",
    "\n",
    "# first tokenize text into words\n",
    "tokens = word_tokenize( text.lower() )\n",
    "print(tokens)\n",
    "tokens_nostop = [w for w in tokens if w not in stopeng]\n",
    "\n",
    "print('\\n', tokens_nostop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "http://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "tokens_porter = [(w, ps.stem(w)) for w in tokens_nostop if w != ps.stem(w)]\n",
    "tokens_porter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download an example tweets file  \n",
    "## https://raw.githubusercontent.com/fivethirtyeight/data/master/trump-twitter/realDonaldTrump_poll_tweets.csv\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "print('Beginning file download with urllib2...')\n",
    "\n",
    "# url at which the file is in direct downloadable format\n",
    "url = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/trump-twitter/realDonaldTrump_poll_tweets.csv'\n",
    "\n",
    "urllib.request.urlretrieve(url, './noisy_twitter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"mark\">**What is `urllib.request.urlretrieve` doing??**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the file with `urlretrieve` and specify the name by which it will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the csv as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first install pandas library and then import it\n",
    "import pandas as pd\n",
    "\n",
    "#read the file into dataframe. \"header=0\" means the first row will be considered as a header\n",
    "frame = pd.read_csv(\"./noisy_twitter.csv\", header=0, dtype={'id':str,\"created_at\":str,'text':str})\n",
    "\n",
    "#print first 5 rows\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print last 5 rows\n",
    "frame.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all text columns\n",
    "\n",
    "textlist = frame['text'].tolist() # here, the order of rows is preserved\n",
    "textlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for calculating top 20 frequent items from a list\n",
    "\n",
    "nltk `most_common`: https://tedboy.github.io/nlps/generated/generated/nltk.FreqDist.most_common.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "def top20(thislist):\n",
    "    # First make a string out of the entire list\n",
    "    BIGstr = \" \".join(thislist)\n",
    "    wordlist = BIGstr.split(\" \")\n",
    "    wordcount = Counter(wordlist)\n",
    "    return(wordcount.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top20(textlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most frequent mentions?\n",
    "\n",
    "Steps:\n",
    "* Extract mentions using regular expressions\n",
    "* Count the most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extractmentions(row):\n",
    "    row = row.lower()\n",
    "    result = re.findall(\"(?<![@\\w])@(\\w{1,25})\", row)\n",
    "    return result\n",
    "\n",
    "all_mentions = []\n",
    "\n",
    "for t in textlist:\n",
    "    result = extractmentions(t)\n",
    "    if len(result) > 0:\n",
    "        all_mentions = all_mentions + result\n",
    "        \n",
    "print(top20(all_mentions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most frequent hashtags?\n",
    "\n",
    "<span class=\"mark\">**TODO**</span>\n",
    "\n",
    "write a similar function to extract hashtags\n",
    "\n",
    "print top 20 most frequent hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 20 words without text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the top words from the entire text corpus\n",
    "top20(textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do some text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcleaner(row):\n",
    "    row = row.lower()\n",
    "    #remove urls\n",
    "    row  = re.sub(r'http\\S+', '', row)\n",
    "    #remove mentions\n",
    "    row = re.sub(r\"(?<![@\\w])@(\\w{1,25})\", '', row)\n",
    "    #remove hashtags\n",
    "    row = re.sub(r\"(?<![#\\w])#(\\w{1,25})\", '',row)\n",
    "    #remove other special characters\n",
    "    row = re.sub('[^A-Za-z .-]+', '', row)\n",
    "    #remove digits\n",
    "    row = re.sub('\\d+', '', row)\n",
    "    row = row.strip(\" \")\n",
    "    return row\n",
    "\n",
    "cleaned_textlist = []\n",
    "\n",
    "for t in textlist:\n",
    "    cleaned_textlist.append(textcleaner(t))\n",
    "    \n",
    "top20(cleaned_textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO for later** There are still few things that could be cleaned up. Like appearance of that last character -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "**<span class=\"mark\">TODO</span>**:\n",
    "1. Fetch english stopwords\n",
    "2. write code to remove stopwords from the text that you are working with: `cleaned_textlist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
